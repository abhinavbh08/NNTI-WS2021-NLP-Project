{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Task1_Word_Embeddings.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"_VZXi_KGi0UR"},"source":["Name -: Abhinav Bhatt <br>\n","Matriculation Number -: 7010946 <br>\n","Email ID -: abbh00001@stud.uni-saarland.de\n","    \n","Name -: Deepa Rani Mahato <br>\n","Matriculation Number -: 7012336 <br>\n","Email ID -: dema00001@stud.uni-saarland.de"]},{"cell_type":"markdown","metadata":{"id":"0DBtlUJ-bi37"},"source":["# Task 1: Word Embeddings (10 points)\n","\n","This notebook will guide you through all steps necessary to train a word2vec model (Detailed description in the PDF)."]},{"cell_type":"markdown","metadata":{"id":"48t-II1vkuau"},"source":["## Imports\n","\n","This code block is reserved for your imports. \n","\n","You are free to use the following packages: \n","\n","(List of packages)"]},{"cell_type":"code","metadata":{"id":"4kh6nh84-AOL"},"source":["# Imports\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import pandas as pd\n","import re # for using regular expressions \n","import torch.nn.functional as F\n","import torch.optim as optim\n","from collections import Counter\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWmk3hVllEcU"},"source":["# 1.1 Get the data (0.5 points)\n","\n","The Hindi portion HASOC corpus from [github.io](https://hasocfire.github.io/hasoc/2019/dataset.html) is already available in the repo, at data/hindi_hatespeech.tsv . Load it into a data structure of your choice. Then, split off a small part of the corpus as a development set (~100 data points).\n","\n","If you are using Colab the first two lines will let you upload folders or files from your local file system."]},{"cell_type":"code","metadata":{"id":"XtI7DJ-0-AOP","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1617136966630,"user_tz":-120,"elapsed":30394,"user":{"displayName":"Abhinav Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifUZGVrEldOUc0dZOUwuXaNMYATdEVO7z1bYYwmw=s64","userId":"11224849619726639130"}},"outputId":"5af53516-86de-4e76-c893-0286441bfc6a"},"source":["#TODO: implement!\n","\n","from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-900515da-d36a-4892-bee3-d1d3a69d9316\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-900515da-d36a-4892-bee3-d1d3a69d9316\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving hindi_hatespeech.tsv to hindi_hatespeech.tsv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YK6gfpHol47o"},"source":["def read_file_and_create_dataframe(uploaded):\n","  \"\"\"Read the uploaded file and create a dataframe from it. Also view some characteristics of the file for initial data exploration.\n","\n","    Args:\n","      uploaded (Dict): Dictionary containing the serialised uploaded file from the colab interface.\n","\n","    Returns:\n","      df (pd.DataFrame): The input file represented as a pandas dataframe.\n","  \"\"\"\n","\n","  # storing the file as a list of lists after splitting it. Since we use colab for uploading, we are using the uploaded object from the previous code cell.\n","  data = uploaded[\"hindi_hatespeech.tsv\"].decode(\"utf-8\").split(\"\\r\\n\")\n","  data = [line.split(\"\\t\") for line in data]\n","\n","  # Viewing some characteristics of the file.\n","  print(data[:10], len(data))\n","\n","  # Converting the file to a pandas dataframe.\n","  header = data.pop(0)\n","  print(header)\n","  df = pd.DataFrame(data, columns=header)\n","\n","  # Viewing some characteristics of the dataframe\n","  print(df.head())\n","  print(df.shape)\n","  print(df['text'].head())\n","\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHM1azlMlq8w","executionInfo":{"status":"ok","timestamp":1617136979190,"user_tz":-120,"elapsed":1088,"user":{"displayName":"Abhinav Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifUZGVrEldOUc0dZOUwuXaNMYATdEVO7z1bYYwmw=s64","userId":"11224849619726639130"}},"outputId":"e359ffb5-d310-4e45-b2b9-202e47ef93b3"},"source":["# Reading the file and creating the dataframe.\n","df = read_file_and_create_dataframe(uploaded)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['text_id', 'text', 'task_1', 'task_2', 'task_3'], ['hasoc_hi_5556', 'बांग्लादेश की शानदार वापसी, भारत को 314 रन पर रोका #INDvBAN #CWC19', 'NOT', 'NONE', 'NONE'], ['hasoc_hi_5648', 'सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांतीदूत के साथ कुछ होगा सब #रंडीरोना शुरू कर देंगे   ', 'HOF', 'PRFN', 'UNT'], ['hasoc_hi_164', 'तुम जैसे हरामियों के लिए बस जूतों की कमी है शुक्र कर अभी तुम्हारी लिंचिंग हुई नहीं है हिंदुओं के जागने की देर है सच में होगी अभी तो तुम जैसे हरामी सुवर ड्रामा बनाएं हो   सुवर कहीं का मौलाना।   तुम जैसे हरामियों कुत्ते की मौत मारना चाहिए सुवर जैसी शक्ल  रंडी की औलाद सुवर कहीं का ।।।।', 'HOF', 'PRFN', 'TIN'], ['hasoc_hi_3530', 'बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत मिलने की खुशी में एक समर्थक ने इंदौर में हर्ष फायरिंग की  #AkashVijayvargiya   https://abpnews.abplive.in/india-news/celebratory-firing-outside-bjp-mla-akash-vijayvargiya-office-in-indore-1157241\\xa0…', 'NOT', 'NONE', 'NONE'], ['hasoc_hi_5206', 'चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर्शन, तेजस्वी यादव नदारद     #biharencephalitisdeaths   https://abpnews.abplive.in/bihar-news/aes-deaths-rjd-protest-in-vidhan-sabha-campus-but-tejashwi-yadav-was-not-present-1158748\\xa0… (रिपोर्ट- @kumarprakash4u )', 'NOT', 'NONE', 'NONE'], ['hasoc_hi_5121', 'मुंबई में बारिश से लोगों को काफी समस्या हो रही है', 'NOT', 'NONE', 'NONE'], ['hasoc_hi_7142', \"Ahmed's dad:-- beta aaj teri mammy kyu nahi baat kr rhi h.   Ahmed.... \", 'NOT', 'NONE', 'NONE'], ['hasoc_hi_4321', '5 लाख मुसलमान उर्स में, अजमेर की दरगाह पर आते हैं, सिर्फ 300 पुलिस वालों के भरोसे,    2 लाख हिंदुओं की अमरनाथ यात्रा के लिए, 80 हजार कमांडो, पैरामिलिट्री फोर्स, तथा करोड़ों के उपकरण लगाए जा रहे हैं    #खतरे में #कौन है ?', 'NOT', 'NONE', 'NONE'], ['hasoc_hi_4674', 'Do mahashaktiyan mili hain, charo taraf khusi ki leher hai, khus hone wale khus hi rhe hain aur bhakton ko taklif ho rhi hai, khair honi bhi chahiye.', 'NOT', 'NONE', 'NONE']] 4666\n","['text_id', 'text', 'task_1', 'task_2', 'task_3']\n","         text_id  ... task_3\n","0  hasoc_hi_5556  ...   NONE\n","1  hasoc_hi_5648  ...    UNT\n","2   hasoc_hi_164  ...    TIN\n","3  hasoc_hi_3530  ...   NONE\n","4  hasoc_hi_5206  ...   NONE\n","\n","[5 rows x 5 columns]\n","(4665, 5)\n","0    बांग्लादेश की शानदार वापसी, भारत को 314 रन पर ...\n","1    सब रंडी नाच देखने मे व्यस्त जैसे ही कोई #शांती...\n","2    तुम जैसे हरामियों के लिए बस जूतों की कमी है शु...\n","3    बीजेपी MLA आकाश विजयवर्गीय जेल से रिहा, जमानत ...\n","4    चमकी बुखार: विधानसभा परिसर में आरजेडी का प्रदर...\n","Name: text, dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sxiy74I4dlvq"},"source":["def split_corpus(df, dev_set=True):\n","  \"\"\"Splits the dataframe into a small development set if dev_set is set to true.\n","\n","    Args:\n","      df (pd.Dataframe): The dataframe containing the data.\n","      dev_set (Boolean): Whether we want to create a subset of the dataframe for initial exploration. Default: True\n","\n","    Returns:\n","      data (pd.DataFrame): If dev_set==True, return a small portion of the data\n","      df (pd.DataFrame): If sev_set==False, return the whole data\n","  \"\"\"\n","\n","\n","  # If we are initially splitting the corpus as development set, then split some small part, else return the full dataframe\n","  if dev_set==True:\n","    data = df.sample(frac=0.022, random_state=88)\n","    return data\n","\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wGGASmdClogh","executionInfo":{"status":"ok","timestamp":1617136983699,"user_tz":-120,"elapsed":387,"user":{"displayName":"Abhinav Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifUZGVrEldOUc0dZOUwuXaNMYATdEVO7z1bYYwmw=s64","userId":"11224849619726639130"}},"outputId":"4065fa86-eb39-4ddf-c52d-e2a37da3bfc2"},"source":["# Initially we set it to True, but then we set it to False for full data training at the end.\n","df = split_corpus(df, dev_set=False)\n","print(df.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(4665, 5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D-mSJ8nUlupB"},"source":["## 1.2 Data preparation (0.5 + 0.5 points)\n","\n","* Prepare the data by removing everything that does not contain information. \n","User names (starting with '@') and punctuation symbols clearly do not convey information, but we also want to get rid of so-called [stopwords](https://en.wikipedia.org/wiki/Stop_word), i. e. words that have little to no semantic content (and, but, yes, the...). Hindi stopwords can be found [here](https://github.com/stopwords-iso/stopwords-hi/blob/master/stopwords-hi.txt) Then, standardize the spelling by lowercasing all words.\n","Do this for the development section of the corpus for now.\n","\n","* What about hashtags (starting with '#') and emojis? Should they be removed too? Justify your answer in the report, and explain how you accounted for this in your implementation."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SoIJZTt3g0SK","executionInfo":{"status":"ok","timestamp":1617136988653,"user_tz":-120,"elapsed":3901,"user":{"displayName":"Abhinav Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifUZGVrEldOUc0dZOUwuXaNMYATdEVO7z1bYYwmw=s64","userId":"11224849619726639130"}},"outputId":"fd6e9113-84f1-4e06-9553-42aa4fde59a4"},"source":["# Downloading the hindi stopwords file.\n","!wget https://raw.githubusercontent.com/stopwords-iso/stopwords-hi/master/stopwords-hi.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-03-30 20:44:30--  https://raw.githubusercontent.com/stopwords-iso/stopwords-hi/master/stopwords-hi.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2663 (2.6K) [text/plain]\n","Saving to: ‘stopwords-hi.txt’\n","\n","\rstopwords-hi.txt      0%[                    ]       0  --.-KB/s               \rstopwords-hi.txt    100%[===================>]   2.60K  --.-KB/s    in 0s      \n","\n","2021-03-30 20:44:30 (40.7 MB/s) - ‘stopwords-hi.txt’ saved [2663/2663]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G2rHY2D7lMVg"},"source":["#TODO: implement!\n","\n","# Performing the data cleaning. \n","# Regular expression for emoji removal taken from https://gist.github.com/Alex-Just/e86110836f3f93fe7932290526529cd1\n","\n","def clean_text(txt, regex_lst, RE_EMOJI, stopwords_hindi):\n","\t\"\"\"This function takes the regular expression for various things such as punctuations and other things which we want to remove and returns the cleaned sentence.\n","\t\t\t\n","\tArgs:\n","\t\ttxt (str): Sentence  which we have to clean.\n","\t\tregex_lst (List[str]): List of all the regular expressions according to whom we have to clean the data.\n","\t\tRE_EMOJI (re): The regular expression for the emojis removal.\n","\t\tstopwords_hindi (List[str]): List of stopwords in Hindi Language\n","\t\t\t\n","\tReturns:\n","\t\tstr_cleaned (str): The cleaned sentence.\n","\t\"\"\"\n","\t  \n","\tstr_cleaned = txt\n","\n","\t# Iterate over the list of regular expressions. \n","\tfor regex in regex_lst:\n","\t\tstr_cleaned = re.sub(regex, '', str_cleaned)\n","\tstr_cleaned = RE_EMOJI.sub(r'', str_cleaned)\n","\n","\tsent_splitted = str_cleaned.split()\n","\n","\t# Do not add the word to the list if it is in the stopwords\n","\tstr_cleaned = \" \".join([x.lower() for x in sent_splitted if x not in stopwords_hindi])\n","\treturn str_cleaned\n","\n","\n","def clean_data_and_remove_stopwords(df, stopwords_file_path):\n","\t\"\"\"Cleans the data and removes the stopwords from the data file. Call the clean_text function for each of the sentence.\n","\n","\tArgs:\n","\t\tdf (pd.DataFrame): The dataframe containing our data.\n","\t\tstopwords_file_path (str): The path of the file containing the stopwords.\n","\n","\tReturns:\n","\t\tdf (pd.DataFrame): The cleaned dataframe with the stopwords removed.\n","\t\"\"\"\n","\n","\t# For removing the emojis\n","\tRE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n","\tregex_lst = [\"@[\\w]+\", # removing the words with @ symbols\n","             \"#[\\w]+\" , # removing the hashtags with the full words which have hashtags\n","             r\"http\\S+\", # removing the urls\n","             r\"[\\\\.,\\/#!$%\\^&\\*;:{}\\।=\\-_`~()\\?]\", # removing the punctuations.\n","             r\"[0-9]\",  # Also remove the numbers\n","             r\"[a-zA-Z]\"] # Also, remove the characters\n","\n","\n","\t# Reading the file containing the stopwords.\n","\twith open(stopwords_file_path, \"r\") as f:\n","\t\tcontent = f.readlines()\n","\n","\t# Store the stopwords in a list\n","\tstopwords_hindi = [x.strip() for x in content]\n","\n","  # Getting the sentences from the dataframe which we have to clean.\n","\tsentences = df['text'].values\n","\tcleaned_sentences = []\n","\n","\t# For each of the sentence, do the regular expressions based data cleaning and add it to the cleaned sentences list\n","\tfor sent in sentences:\n","\t\tcleaned_sentences.append(clean_text(sent, regex_lst, RE_EMOJI, stopwords_hindi))\n","\n","\tdf[\"text\"] = cleaned_sentences\n","\n","\t# Finding the indices of the sentences where we have one or less than one word\n","\tindices_to_drop = []\n","\tfor i, item in enumerate(df[\"text\"].values):\n","\t\tif len(item.split()) <= 1:\n","\t\t\tindices_to_drop.append(i)\n","\n","\t# Dropping the indices for the sentences which have one or less than one word.\n","\tdf.drop(df.index[indices_to_drop], inplace=True)\n","\tdf.reset_index(inplace=True, drop=True)\n","\n","\treturn df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JtQRPpVmlmWt"},"source":["df = clean_data_and_remove_stopwords(df, stopwords_file_path=\"stopwords-hi.txt\")\n","stopwords_removed_sentences = list(df[\"text\"].values)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Je09nozLmmMm"},"source":["## 1.3 Build the vocabulary (0.5 + 0.5 points)\n","\n","The input to the first layer of word2vec is an one-hot encoding of the current word. The output od the model is then compared to a numeric class label of the words within the size of the skip-gram window. Now\n","\n","* Compile a list of all words in the development section of your corpus and save it in a variable ```V```."]},{"cell_type":"code","metadata":{"id":"VpoGmTKx-AOQ"},"source":["#TODO: implement!\n","\n","def create_vocab(stopwords_removed_sentences):\n","  \"\"\"Create the vocab and corresponding dictionaries for indices to word and word to indices conversion.\n","\n","  Args:\n","    stopwords_removed_sentences (List[str]): The list of clean sentences.\n","\n","  Returns:\n","    V (List[str]): The list containing all the unique words in our data\n","    word2idx (Dict): The dictionary containing mapping from word to indices.\n","    idx2word (Dict): The dictionary containing mapping from indices to words.\n","  \"\"\"\n","  \n","  V = []\n","  for sent in stopwords_removed_sentences:\n","    tokens = sent.split() # Get each of the token in the vocab\n","    for token in tokens:\n","      if token not in V: # Add the token to the vocab list if it is not there already.\n","        V.append(token)\n","\n","  # Creating dictionaries for converting word to unique indices and converting words back from the unique indices. \n","  word2idx = {word:idx for idx, word in enumerate(V)}\n","  idx2word = {idx:word for idx, word in enumerate(V)}\n","  \n","  return V, word2idx, idx2word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rQXGJGQLpmEN","executionInfo":{"status":"ok","timestamp":1617099496468,"user_tz":-120,"elapsed":6139,"user":{"displayName":"Abhinav Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifUZGVrEldOUc0dZOUwuXaNMYATdEVO7z1bYYwmw=s64","userId":"11224849619726639130"}},"outputId":"b2b8afd2-5b28-489e-bc2b-e8c1787e1adb"},"source":["# Create the vocabulary\n","V, word2idx, idx2word = create_vocab(stopwords_removed_sentences)\n","len(V), len(word2idx), len(idx2word)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15671, 15671, 15671)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"WiaVglVNoENY"},"source":["* Then, write a function ```word_to_one_hot``` that returns a one-hot encoding of an arbitrary word in the vocabulary. The size of the one-hot encoding should be ```len(v)```."]},{"cell_type":"code","metadata":{"id":"yqPNw6IT-AOQ"},"source":["#TODO: implement!\n","def word_to_one_hot(word, word2idx):\n","  \"\"\"Return the one hot encoding of the word given to this function. \n","  \n","    Args:\n","      word (str): The word for which one hot representation is required.\n","      word2idx (Dict): The dictionary mapping from words to indices.\n","\n","    Returns:\n","      x (torch.Tensor): The one hot representation for the word.\n","  \"\"\"\n","\n","  # Create a vector or zeros equal to the length of the vocab. len(word2idx) is same as len(V) because word2idx is created from V.\n","  x = torch.zeros(len(word2idx)).float() \n","  \n","  # Setting the value corresponding to the index of word 1\n","  x[word2idx[word]] = 1.0 \n","  \n","  return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gKD8zBlxVclh"},"source":["## 1.4 Subsampling (0.5 points)\n","\n","The probability to keep a word in a context is given by:\n","\n","$P_{keep}(w_i) = \\Big(\\sqrt{\\frac{z(w_i)}{0.000001}}+1\\Big) \\cdot \\frac{0.000001}{z(w_i)}$\n","\n","Where $z(w_i)$ is the relative frequency of the word $w_i$ in the corpus. Now,\n","* Calculate word frequencies\n","* Define a function ```sampling_prob``` that takes a word (string) as input and returns the probabiliy to **keep** the word in a context."]},{"cell_type":"code","metadata":{"id":"Mj4sDOVMMr0b"},"source":["#TODO: implement!\n","\n","def sampling_prob(word, probs):\n","\n","  \"\"\" This function gives the sampling probability of the word using the probabilities which we will pre calculate in the next function. \"\"\"\n","\n","  return probs[word]\n","\n","def pre_calculate_probabilities(stopwords_removed_sentences):\n","  \"\"\"Calculate the probability to keep a word in context for all the words in the corpus.\n","\n","    Args:\n","      stopwords_removed_sentences (List[str]): The cleaned sentences.\n","\n","    Returns:\n","      probs (Dict): The dictionary containing word as key and probability to keep it in a context as value.\n","  \"\"\"\n","\n","  # all tokens contain all the tokens in the corpus, even if they are repeated. We will use it next for calculating the word frequencies.\n","  all_tokens = [] \n","\n","  # Add all of the tokens in the vocab.\n","  for sent in stopwords_removed_sentences:\n","    tokens = sent.split()\n","    for token in tokens:\n","      all_tokens.append(token)\n","\n","  # Get the counts of all the tokens in the vocab.\n","  counts = Counter(all_tokens) \n","\n","  # Find the relative word frequencies.\n","  freqs = {}\n","  for word, count in counts.items():\n","    freqs[word] = count / len(all_tokens)\n","\n","  # Find the probability to keep a word in the context using the above given formula. # As told in the course discussion forum, 0.000001 should be taken instead of 0.001.\n","  probs = {}\n","  for word, freq in freqs.items():\n","    probs[word] = (np.sqrt(freq/0.000001) + 1)*(0.000001/freq)\n","\n","  return probs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NydszY6pxIJy"},"source":["# Pre calculate the probability to keep a word in the context for each of the word.\n","probs = pre_calculate_probabilities(stopwords_removed_sentences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"_J9f5YM6R658","executionInfo":{"status":"ok","timestamp":1617100009437,"user_tz":-120,"elapsed":1096,"user":{"displayName":"Abhinav Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifUZGVrEldOUc0dZOUwuXaNMYATdEVO7z1bYYwmw=s64","userId":"11224849619726639130"}},"outputId":"c2838b84-0c94-4e82-f31d-a40e839c745a"},"source":["import matplotlib.pyplot as plt\n","\n","# Plot a histogram of the probabilities of keeping the words in the context.\n","plt.hist(list(probs.values()))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([ 279.,  646.,  660.,  974.,  615., 1035., 2298.,    0.,    0.,\n","        9164.]),\n"," array([0.01130212, 0.04390623, 0.07651033, 0.10911444, 0.14171854,\n","        0.17432265, 0.20692676, 0.23953086, 0.27213497, 0.30473908,\n","        0.33734318]),\n"," <a list of 10 Patch objects>)"]},"metadata":{"tags":[]},"execution_count":17},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"kxV1P90zplxu"},"source":["# 1.5 Skip-Grams (1 point)\n","\n","Now that you have the vocabulary and one-hot encodings at hand, you can start to do the actual work. The skip gram model requires training data of the shape ```(current_word, context)```, with ```context``` being the words before and/or after ```current_word``` within ```window_size```. \n","\n","* Have closer look on the original paper. If you feel to understand how skip-gram works, implement a function ```get_target_context``` that takes a sentence as input and [yield](https://docs.python.org/3.9/reference/simple_stmts.html#the-yield-statement)s a ```(current_word, context)```.\n","\n","* Use your ```sampling_prob``` function to drop words from contexts as you sample them. "]},{"cell_type":"code","metadata":{"id":"r8CCTpVy-AOR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617101147548,"user_tz":-120,"elapsed":3550,"user":{"displayName":"Abhinav Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifUZGVrEldOUc0dZOUwuXaNMYATdEVO7z1bYYwmw=s64","userId":"11224849619726639130"}},"outputId":"2ccba04a-fb93-4000-c3ba-1b0eed22b80e"},"source":["#TODO: implement!\n","\n","# Word2vec paper recommends window size of 10, but we keep here to 5 as keeping size 10 leads to a lot of noise due to the small size of the dataset.\n","window_size = 5\n","\n","def get_target_context(sentence, window_size, word2idx, idx2word, probs):\n","  \"\"\"Yield a pair of (current_word, context) and also sample the context word on the basis of the probability to keep it in a context.\n","  \n","    Args:\n","      sentence (str): The sentence for which we want to generate the (current_word, context) pair.\n","      window_size (int): The size of the context windows befor and after a word.\n","      word2idx (Dict): The dictionary mapping from words to indices.\n","      idx2word (Dict): The dictionary mapping from indices to words.\n","      probs (Dict): Dictionary containing a word and the probability to keep it in a context.\n","\n","    Returns:\n","      (current_word, context) (Tuple): The current word and the context for each pair in the sentence.\n","  \"\"\"\n","  \n","  # Making a list of words in the sentence.\n","  words = sentence.split()\n","\n","  # Take every word in the current sentence as the centre word.\n","  for center_word in range(0, len(words)): \n","\n","    # Go through each word in the window_size around the current word.\n","    for current_window in range(-window_size, window_size+1): \n","      context_word = center_word + current_window\n","\n","      # If word is out of bounds, or equal to center word, do not yield it.\n","      if center_word==context_word or context_word<0 or context_word>=len(words):\n","        continue\n","\n","      # Keeping this range because maximum probability to keep in context for our dataset is 0.35.\n","      p = np.random.uniform(0, 0.35)\n","\n","      # Do not remove words which occur one time.\n","      if p<0.3:\n","        # Sampling the word based on its sampling probability.\n","        if sampling_prob(words[context_word], probs) < p:\n","          continue\n","\n","      # Yield the (current_word, context) pair.\n","      yield (words[center_word], words[context_word])\n","\n","\n","# List to store all the (current_word, context) pairs.\n","word_pairs = [] \n","\n","for sentence in stopwords_removed_sentences:  \n","  # For each of the sentence, get the traget context from the above generator\n","  for pair in get_target_context(sentence, window_size, word2idx, idx2word, probs):\n","    word_pairs.append(pair)\n","\n","for j in range(0, 10):\n","  print(word_pairs[j][0], word_pairs[j][1]) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["बांग्लादेश रन\n","बांग्लादेश रोका\n","शानदार बांग्लादेश\n","शानदार रन\n","वापसी रोका\n","भारत बांग्लादेश\n","रन भारत\n","रन रोका\n","रोका शानदार\n","सब व्यस्त\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3nLk1lSwWC_c","executionInfo":{"status":"ok","timestamp":1617101150136,"user_tz":-120,"elapsed":577,"user":{"displayName":"Abhinav Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifUZGVrEldOUc0dZOUwuXaNMYATdEVO7z1bYYwmw=s64","userId":"11224849619726639130"}},"outputId":"27e1e2bd-2fa3-4339-a2ee-605605ea4812"},"source":["len(word_pairs)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["259673"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"gfEFgtkmuDjL"},"source":["# 1.6 Hyperparameters (0.5 points)\n","\n","According to the word2vec paper, what would be a good choice for the following hyperparameters? \n","\n","* Embedding dimension\n","* Window size\n","\n","Initialize them in a dictionary or as independent variables in the code block below. "]},{"cell_type":"code","metadata":{"id":"d7xSKuFJcYoD"},"source":["# Set hyperparameters\n","\n","# Word2vec paper recommends window size of 10, but we keep here to 5 as keeping size 10 leads to a lot of noise due to the small size of the dataset.\n","window_size = 5\n","embedding_size = 300 \n","\n","# More hyperparameters\n","learning_rate = 0.03\n","epochs = 300"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xiM2zq-YunPx"},"source":["# 1.7 Pytorch Module (0.5 + 0.5 + 0.5 points)\n","\n","Pytorch provides a wrapper for your fancy and super-complex models: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The code block below contains a skeleton for such a wrapper. Now,\n","\n","* Initialize the two weight matrices of word2vec as fields of the class.\n","\n","* Override the ```forward``` method of this class. It should take a one-hot encoding as input, perform the matrix multiplications, and finally apply a log softmax on the output layer.\n","\n","* Initialize the model and save its weights in a variable. The Pytorch documentation will tell you how to do that."]},{"cell_type":"code","metadata":{"id":"D9sGNytYhwxS"},"source":["# Create model \n","torch.manual_seed(88)\n","vocabulary_size = len(V)\n","\n","class Word2Vec(nn.Module):\n","  def __init__(self, vocabulary_size, embedding_size):\n","    super(Word2Vec, self).__init__()\n","\n","    self.vocabulary_size = vocabulary_size\n","    self.embedding_size = embedding_size\n","    # Weight matrix for input to hidden layer\n","    self.w1 = nn.Parameter(torch.randn(vocabulary_size, embedding_size, requires_grad=True)) \n","\n","    # Weight matrix for hidden to output layer.\n","    self.w2 = nn.Parameter(torch.randn(embedding_size, vocabulary_size, requires_grad=True)) \n","\n","  def forward(self, one_hot):\n","    # Doing the required matrix multiplications\n","    z1 = torch.matmul(one_hot, self.w1)\n","    z2 = torch.matmul(z1, self.w2)\n","\n","    # Taking log softmax on the output layer.\n","    log_softmax = F.log_softmax(z2, dim=1)\n","    \n","    return log_softmax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGy510psi4f9"},"source":["# Initializing the model\n","model = Word2Vec(vocabulary_size, embedding_size) \n","\n","# Saving its weights in a variable.\n","torch.save(model.state_dict(), 'weights_initial.pth') "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XefIDMMHv5zJ"},"source":["# 1.8 Loss function and optimizer (0.5 points)\n","\n","Initialize variables with [optimizer](https://pytorch.org/docs/stable/optim.html#module-torch.optim) and loss function. You can take what is used in the word2vec paper, but you can use alternative optimizers/loss functions if you explain your choice in the report."]},{"cell_type":"code","metadata":{"id":"V9-Ino-e29w3"},"source":["# Define optimizer and loss\n","\n","# SGD as used by the original paper\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n","\n","# Using Negative log likelihood loss on the output from the forward pass.\n","criterion = nn.NLLLoss() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ckTfK78Ew8wI"},"source":["# 1.9 Training the model (3 points)\n","\n","As everything is prepared, implement a training loop that performs several passes of the data set through the model. You are free to do this as you please, but your code should:\n","\n","* Load the weights saved in 1.6 at the start of every execution of the code block\n","* Print the accumulated loss at least after every epoch (the accumulate loss should be reset after every epoch)\n","* Define a criterion for the training procedure to terminate if a certain loss value is reached. You can find the threshold by observing the loss for the development set.\n","\n","You can play around with the number of epochs and the learning rate."]},{"cell_type":"code","metadata":{"id":"VnFunQbNVofe"},"source":["def get_input_layer_onehot(data, word2idx):\n","  \"\"\"Get matrix of one hot encoded input data by calling word_to_one_hot function defined earlier\n","\n","      Args:\n","        data (List[str]): The list of words to one hot encode.\n","        word2idx (Dict): Dictionary mapping from words to indices.\n","\n","      Returns:\n","        mat (torch.Tensor): A matrix containing the one hot representations for all the words in data list.\n","  \"\"\"\n","\n","  data_points = len(data)\n","\n","  # Initiaise the matrix as zeros.\n","  mat = torch.zeros(data_points, len(word2idx))\n","\n","  # For each of the data point, add it to the matrix after getting its one hot representation by calling word_to_one_hot function defined earlier.\n","  for i, index in enumerate(data):\n","    mat[i] = word_to_one_hot(index, word2idx)\n","\n","  return mat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uRFWw7QEpEb5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616597470461,"user_tz":-60,"elapsed":737,"user":{"displayName":"Abhinav Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifUZGVrEldOUc0dZOUwuXaNMYATdEVO7z1bYYwmw=s64","userId":"11224849619726639130"}},"outputId":"2cf47bdb-d6c7-49db-9ba2-c2d5bd2f6434"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device) # Check whether we there is GPU or not"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LbMGD5L0mLDx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616617713337,"user_tz":-60,"elapsed":20241240,"user":{"displayName":"Abhinav Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifUZGVrEldOUc0dZOUwuXaNMYATdEVO7z1bYYwmw=s64","userId":"11224849619726639130"}},"outputId":"094cdb1a-9f39-415c-834e-1edaf38d0f0b"},"source":["# Define train procedure\n","\n","# load initial weights\n","model.load_state_dict(torch.load('weights_initial.pth'))\n","model = model.to(device)\n","batch_size = 16\n","word_pairs = np.array(word_pairs)\n","\n","def train():\n"," \n","  print(\"Training started\")\n","\n","  for epoch in range(epochs):\n","    loss_val = 0\n","\n","    # for each batch in the dataloaders\n","    for data, target in zip(DataLoader(word_pairs[:, 0], batch_size=batch_size, shuffle=False, num_workers=1, drop_last=True), DataLoader(word_pairs[:, 1], batch_size=batch_size, shuffle=False, num_workers=1, drop_last=True)):\n","\n","      # Clear out the gradients from the previous batch\n","      optimizer.zero_grad()\n","\n","      # Get the one hot representation for the input words in the batch.\n","      x = get_input_layer_onehot(data, word2idx).to(device)\n","\n","      # Get the indices for the target context words in the batch\n","      y_true = torch.from_numpy(np.array([word2idx[t] for t in target])).long().to(device)\n","\n","  \t\t# Do the forward pass of the model\n","      y_pred = model(x)\n","\n","      # calculate the loss value using our loss function on this batch\n","      loss = criterion(y_pred, y_true)\n","      loss_val = loss_val + loss.item()\n","\n","  \t\t# Do backpropagation of the gradients\n","      loss.backward()\n","  \n","      # Update the weights\n","      optimizer.step()\n","\n","    loss_epoch = loss_val/(len(word_pairs)/batch_size)\n","\n","    Criterion for the training procesure to terminate if a certain loss value is reached.\n","    if loss_epoch<4.5:\n","      break\n","\n","    # Print the loss after every epoch.\n","    print(f'Loss at epo {epoch}: {loss_epoch}')\n","\n","# Train the model\n","train()\n","\n","print(\"Training finished\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training started\n","Loss at epo 0: 58.75952827089474\n","Loss at epo 1: 50.33309435133063\n","Loss at epo 2: 45.498194260727296\n","Loss at epo 3: 41.890520104183956\n","Loss at epo 4: 38.98281176576999\n","Loss at epo 5: 36.529466516056395\n","Loss at epo 6: 34.408891664042805\n","Loss at epo 7: 32.54298823990994\n","Loss at epo 8: 30.876073382282858\n","Loss at epo 9: 29.368241219772234\n","Loss at epo 10: 27.9931508517617\n","Loss at epo 11: 26.730285687773318\n","Loss at epo 12: 25.56263842429771\n","Loss at epo 13: 24.47728821703145\n","Loss at epo 14: 23.464347105960595\n","Loss at epo 15: 22.51568787093713\n","Loss at epo 16: 21.624650150737477\n","Loss at epo 17: 20.78549817996529\n","Loss at epo 18: 19.99306133244842\n","Loss at epo 19: 19.243229911037957\n","Loss at epo 20: 18.532375761203536\n","Loss at epo 21: 17.857624691955685\n","Loss at epo 22: 17.216381369634806\n","Loss at epo 23: 16.606206154813837\n","Loss at epo 24: 16.024787215855113\n","Loss at epo 25: 15.470142850456924\n","Loss at epo 26: 14.94052198070148\n","Loss at epo 27: 14.434603266954618\n","Loss at epo 28: 13.951168396888047\n","Loss at epo 29: 13.489078641294817\n","Loss at epo 30: 13.04708133094341\n","Loss at epo 31: 12.624120553949025\n","Loss at epo 32: 12.219415118442173\n","Loss at epo 33: 11.832193125804435\n","Loss at epo 34: 11.46174380263065\n","Loss at epo 35: 11.107401174532724\n","Loss at epo 36: 10.768521704021294\n","Loss at epo 37: 10.444534558063735\n","Loss at epo 38: 10.1349738060893\n","Loss at epo 39: 9.839327597568756\n","Loss at epo 40: 9.557094291732268\n","Loss at epo 41: 9.287830326493909\n","Loss at epo 42: 9.03109152629205\n","Loss at epo 43: 8.786403458233316\n","Loss at epo 44: 8.55335239699362\n","Loss at epo 45: 8.331579165786248\n","Loss at epo 46: 8.120664999682194\n","Loss at epo 47: 7.920405283636511\n","Loss at epo 48: 7.730482804333545\n","Loss at epo 49: 7.550670819397823\n","Loss at epo 50: 7.380557432324177\n","Loss at epo 51: 7.219895513565327\n","Loss at epo 52: 7.068290776473871\n","Loss at epo 53: 6.925419275572091\n","Loss at epo 54: 6.790880908676486\n","Loss at epo 55: 6.664371423012951\n","Loss at epo 56: 6.545460405193768\n","Loss at epo 57: 6.433878013426374\n","Loss at epo 58: 6.329259837295911\n","Loss at epo 59: 6.231361073343092\n","Loss at epo 60: 6.139802084777166\n","Loss at epo 61: 6.054353248927513\n","Loss at epo 62: 5.974689966127597\n","Loss at epo 63: 5.900602695464044\n","Loss at epo 64: 5.831741097690687\n","Loss at epo 65: 5.767891255932199\n","Loss at epo 66: 5.708719493694513\n","Loss at epo 67: 5.653966751155681\n","Loss at epo 68: 5.603282637578983\n","Loss at epo 69: 5.556503500485906\n","Loss at epo 70: 5.513317987897323\n","Loss at epo 71: 5.4735010978611305\n","Loss at epo 72: 5.436805392033755\n","Loss at epo 73: 5.403041014831793\n","Loss at epo 74: 5.371994402947407\n","Loss at epo 75: 5.343497631546132\n","Loss at epo 76: 5.31729003510058\n","Loss at epo 77: 5.293256604282349\n","Loss at epo 78: 5.271165542327001\n","Loss at epo 79: 5.250858494004851\n","Loss at epo 80: 5.232085190410698\n","Loss at epo 81: 5.214797296896664\n","Loss at epo 82: 5.198811707861668\n","Loss at epo 83: 5.184039390329684\n","Loss at epo 84: 5.170352565189884\n","Loss at epo 85: 5.157683280576921\n","Loss at epo 86: 5.1459167281622875\n","Loss at epo 87: 5.134936026953579\n","Loss at epo 88: 5.1246999894844345\n","Loss at epo 89: 5.115142482305931\n","Loss at epo 90: 5.1061822765030564\n","Loss at epo 91: 5.097760531075875\n","Loss at epo 92: 5.089837834394022\n","Loss at epo 93: 5.082358589649187\n","Loss at epo 94: 5.075288708072783\n","Loss at epo 95: 5.068557809311138\n","Loss at epo 96: 5.062169454703583\n","Loss at epo 97: 5.056101665884917\n","Loss at epo 98: 5.05030062408095\n","Loss at epo 99: 5.044706286593075\n","Loss at epo 100: 5.03933593533922\n","Loss at epo 101: 5.0341702236258214\n","Loss at epo 102: 5.0291658103080765\n","Loss at epo 103: 5.024322607071304\n","Loss at epo 104: 5.019615251075583\n","Loss at epo 105: 5.015001203742119\n","Loss at epo 106: 5.010513218907919\n","Loss at epo 107: 5.006148197582659\n","Loss at epo 108: 5.001874866937057\n","Loss at epo 109: 4.997669598268953\n","Loss at epo 110: 4.993555611179084\n","Loss at epo 111: 4.989536016479135\n","Loss at epo 112: 4.985587176733255\n","Loss at epo 113: 4.9816643328582275\n","Loss at epo 114: 4.977820131975529\n","Loss at epo 115: 4.974046674321108\n","Loss at epo 116: 4.970317011647354\n","Loss at epo 117: 4.966651316812447\n","Loss at epo 118: 4.963009048496202\n","Loss at epo 119: 4.959435794100471\n","Loss at epo 120: 4.955852110595865\n","Loss at epo 121: 4.952342219091134\n","Loss at epo 122: 4.948861716908267\n","Loss at epo 123: 4.94543649560637\n","Loss at epo 124: 4.941982745582088\n","Loss at epo 125: 4.938599150195529\n","Loss at epo 126: 4.93523472999723\n","Loss at epo 127: 4.931910361870632\n","Loss at epo 128: 4.928604542686883\n","Loss at epo 129: 4.9253378792098585\n","Loss at epo 130: 4.922088376915556\n","Loss at epo 131: 4.918884904339024\n","Loss at epo 132: 4.915649267207087\n","Loss at epo 133: 4.912473772887745\n","Loss at epo 134: 4.90931342303285\n","Loss at epo 135: 4.906186891775518\n","Loss at epo 136: 4.903070460628591\n","Loss at epo 137: 4.899995439473377\n","Loss at epo 138: 4.896886335225004\n","Loss at epo 139: 4.893831657918663\n","Loss at epo 140: 4.890790353041549\n","Loss at epo 141: 4.887780946187851\n","Loss at epo 142: 4.8847628541994705\n","Loss at epo 143: 4.88178797667325\n","Loss at epo 144: 4.8787955324137044\n","Loss at epo 145: 4.875850965444763\n","Loss at epo 146: 4.872894118934344\n","Loss at epo 147: 4.869980767702217\n","Loss at epo 148: 4.867047753503665\n","Loss at epo 149: 4.864160419167945\n","Loss at epo 150: 4.861263027662472\n","Loss at epo 151: 4.858407538800986\n","Loss at epo 152: 4.8555232979860055\n","Loss at epo 153: 4.85268434688484\n","Loss at epo 154: 4.84985178006103\n","Loss at epo 155: 4.84705337438224\n","Loss at epo 156: 4.8442073194739494\n","Loss at epo 157: 4.841411986981394\n","Loss at epo 158: 4.838620281771955\n","Loss at epo 159: 4.835851936755109\n","Loss at epo 160: 4.833086731294675\n","Loss at epo 161: 4.830347134342331\n","Loss at epo 162: 4.8276187456426936\n","Loss at epo 163: 4.824866187685648\n","Loss at epo 164: 4.822141455741662\n","Loss at epo 165: 4.819440626677481\n","Loss at epo 166: 4.816745047650198\n","Loss at epo 167: 4.814061349693373\n","Loss at epo 168: 4.811393390275837\n","Loss at epo 169: 4.8087077219598795\n","Loss at epo 170: 4.806047399560477\n","Loss at epo 171: 4.8034128368032025\n","Loss at epo 172: 4.800788355615617\n","Loss at epo 173: 4.798134012384062\n","Loss at epo 174: 4.795507869457198\n","Loss at epo 175: 4.792901500704301\n","Loss at epo 176: 4.790295254105465\n","Loss at epo 177: 4.7877084668486445\n","Loss at epo 178: 4.785120872930979\n","Loss at epo 179: 4.782556909119628\n","Loss at epo 180: 4.779998209849867\n","Loss at epo 181: 4.777433466953496\n","Loss at epo 182: 4.774887858172035\n","Loss at epo 183: 4.772350870757545\n","Loss at epo 184: 4.769831689723593\n","Loss at epo 185: 4.767287265555702\n","Loss at epo 186: 4.76476692104401\n","Loss at epo 187: 4.762269369953883\n","Loss at epo 188: 4.759776080949539\n","Loss at epo 189: 4.7572828920665335\n","Loss at epo 190: 4.754805813169382\n","Loss at epo 191: 4.75232550668785\n","Loss at epo 192: 4.749865838692316\n","Loss at epo 193: 4.747397939522941\n","Loss at epo 194: 4.744948857688353\n","Loss at epo 195: 4.742508989288592\n","Loss at epo 196: 4.740084826581206\n","Loss at epo 197: 4.737634167962345\n","Loss at epo 198: 4.735206908367189\n","Loss at epo 199: 4.732801314349028\n","Loss at epo 200: 4.730397960456688\n","Loss at epo 201: 4.727999828385835\n","Loss at epo 202: 4.725615225164042\n","Loss at epo 203: 4.723217092600633\n","Loss at epo 204: 4.720840021614724\n","Loss at epo 205: 4.718477125403502\n","Loss at epo 206: 4.716125736919461\n","Loss at epo 207: 4.713745736301867\n","Loss at epo 208: 4.71139001337767\n","Loss at epo 209: 4.709054006851274\n","Loss at epo 210: 4.706716878442449\n","Loss at epo 211: 4.704399953246529\n","Loss at epo 212: 4.702090142767499\n","Loss at epo 213: 4.699752514560654\n","Loss at epo 214: 4.69743845952939\n","Loss at epo 215: 4.695144994787083\n","Loss at epo 216: 4.69285145393374\n","Loss at epo 217: 4.6905716316226\n","Loss at epo 218: 4.68830153557868\n","Loss at epo 219: 4.6860068733869245\n","Loss at epo 220: 4.683734716301977\n","Loss at epo 221: 4.681486176156643\n","Loss at epo 222: 4.679241461949158\n","Loss at epo 223: 4.67698105801696\n","Loss at epo 224: 4.674738586353971\n","Loss at epo 225: 4.672518965653462\n","Loss at epo 226: 4.670307556836218\n","Loss at epo 227: 4.66806043090204\n","Loss at epo 228: 4.665839763966989\n","Loss at epo 229: 4.663638010782863\n","Loss at epo 230: 4.6614378700382355\n","Loss at epo 231: 4.659259299219031\n","Loss at epo 232: 4.657061916924257\n","Loss at epo 233: 4.654899020134118\n","Loss at epo 234: 4.65270674429964\n","Loss at epo 235: 4.650554366226936\n","Loss at epo 236: 4.648386073411973\n","Loss at epo 237: 4.646252555225076\n","Loss at epo 238: 4.644081382607521\n","Loss at epo 239: 4.641950546891769\n","Loss at epo 240: 4.6398215582857\n","Loss at epo 241: 4.637720391259681\n","Loss at epo 242: 4.635565470741616\n","Loss at epo 243: 4.633456654479117\n","Loss at epo 244: 4.631345296991621\n","Loss at epo 245: 4.629253245521326\n","Loss at epo 246: 4.627158218508648\n","Loss at epo 247: 4.6250857514750665\n","Loss at epo 248: 4.623018368545588\n","Loss at epo 249: 4.620924970013583\n","Loss at epo 250: 4.61885336348391\n","Loss at epo 251: 4.616803868420022\n","Loss at epo 252: 4.614756177238454\n","Loss at epo 253: 4.6127073225864965\n","Loss at epo 254: 4.610672480025833\n","Loss at epo 255: 4.608635025142922\n","Loss at epo 256: 4.606615719482067\n","Loss at epo 257: 4.6045858878477555\n","Loss at epo 258: 4.602573224027302\n","Loss at epo 259: 4.600573387581652\n","Loss at epo 260: 4.5985856467406325\n","Loss at epo 261: 4.596567649148642\n","Loss at epo 262: 4.594573055660306\n","Loss at epo 263: 4.5925968793228185\n","Loss at epo 264: 4.59061692685452\n","Loss at epo 265: 4.588659082455572\n","Loss at epo 266: 4.5867028634055185\n","Loss at epo 267: 4.584739428113083\n","Loss at epo 268: 4.582791592885338\n","Loss at epo 269: 4.580851019943001\n","Loss at epo 270: 4.578924771218522\n","Loss at epo 271: 4.576972832405332\n","Loss at epo 272: 4.5750415381137985\n","Loss at epo 273: 4.57313234171701\n","Loss at epo 274: 4.571224711768157\n","Loss at epo 275: 4.56931285189674\n","Loss at epo 276: 4.5674149811869995\n","Loss at epo 277: 4.565520119814912\n","Loss at epo 278: 4.56364113512589\n","Loss at epo 279: 4.561740976813743\n","Loss at epo 280: 4.559859508374474\n","Loss at epo 281: 4.558002483414367\n","Loss at epo 282: 4.556150167208103\n","Loss at epo 283: 4.554268283962576\n","Loss at epo 284: 4.552407928561956\n","Loss at epo 285: 4.550566295028345\n","Loss at epo 286: 4.548721336884033\n","Loss at epo 287: 4.5468983494813235\n","Loss at epo 288: 4.545079103259953\n","Loss at epo 289: 4.54323466868136\n","Loss at epo 290: 4.541410378848459\n","Loss at epo 291: 4.539608683054582\n","Loss at epo 292: 4.537809029819828\n","Loss at epo 293: 4.5359968687178345\n","Loss at epo 294: 4.53420011009178\n","Loss at epo 295: 4.53242075722858\n","Loss at epo 296: 4.530650640046127\n","Loss at epo 297: 4.52884503453565\n","Loss at epo 298: 4.5270639762794165\n","Loss at epo 299: 4.525300451358223\n","Training finished\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BgQkaYstyj0Q"},"source":["# 1.10 Train on the full dataset (0.5 points)\n","\n","Now, go back to 1.1 and remove the restriction on the number of sentences in your corpus. Then, reexecute code blocks 1.2, 1.3 and 1.6 (or those relevant if you created additional ones). \n","\n","* Then, retrain your model on the complete dataset.\n","\n","* Now, the input weights of the model contain the desired word embeddings! Save them together with the corresponding vocabulary items (Pytorch provides a nice [functionality](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for this)."]},{"cell_type":"code","metadata":{"id":"jZUhVXor_Kmb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616617713368,"user_tz":-60,"elapsed":20240499,"user":{"displayName":"Abhinav Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifUZGVrEldOUc0dZOUwuXaNMYATdEVO7z1bYYwmw=s64","userId":"11224849619726639130"}},"outputId":"2feca4ad-cacf-4df3-d145-82c70142651b"},"source":["# Getting the weights from the matrices\n","\n","w1 = model.w1.data\n","w2 = model.w2.data\n","\n","print(w1.size(), w2.size())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([15671, 300]) torch.Size([300, 15671])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4x8hQP_bg4_g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616617713390,"user_tz":-60,"elapsed":20239539,"user":{"displayName":"Abhinav Bhatt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GifUZGVrEldOUc0dZOUwuXaNMYATdEVO7z1bYYwmw=s64","userId":"11224849619726639130"}},"outputId":"f18d43ca-80a5-460c-c4f5-7fbcd15dba11"},"source":["# Transferring the matrices to cpu and then converting them to numpy arrays.\n","\n","w1 = w1.cpu().detach().numpy()\n","w2 = w2.T.cpu().detach().numpy()\n","print(w1.shape, w2.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(15671, 300) (15671, 300)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"99c7DuxbAQUa"},"source":["# Write the embeddings to a text file for matrix w1 in the form in which word2vec and glove embeddings are stored.\n","\n","with open(\"embeddings_hindi_2.txt\", 'w') as f:\n","    for i, embedding in enumerate(w1):\n","        word = V[i]\n","        vector = ' '.join([str(i) for i in embedding.tolist()])\n","        f.write(f'{word} {vector}\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKfoAPWzxhWZ"},"source":[""],"execution_count":null,"outputs":[]}]}